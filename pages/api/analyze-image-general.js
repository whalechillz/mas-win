import OpenAI from 'openai';
import { logOpenAIUsage } from '../../lib/ai-usage-logger';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ message: 'Method not allowed' });
  }

  try {
    const { imageUrl, title, excerpt } = req.body;

    if (!imageUrl) {
      return res.status(400).json({ message: 'Image URL is required' });
    }

    console.log('ğŸ” ë²”ìš© ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì‹œì‘:', imageUrl);
    console.log('ğŸ”§ OpenAI API í‚¤ í™•ì¸:', process.env.OPENAI_API_KEY ? 'ì„¤ì •ë¨' : 'ëˆ„ë½');

    // OpenAI Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ìš© ì´ë¯¸ì§€ ë¶„ì„ (ëª¨ë“  ë©”íƒ€ë°ì´í„° í•œ ë²ˆì— ìƒì„±)
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: `You are an expert image analyzer for general content. 
Analyze the given image and generate all metadata in JSON format.

Guidelines:
- Write in Korean language
- Generate all metadata fields: alt_text, title, description, keywords
- Focus on visual elements: composition, lighting, colors, objects, people, setting
- Include specific details if present (buildings, food, people, landscapes, products, etc.)
- Use descriptive adjectives and natural Korean expressions
- Be rich, detailed, and vivid in your descriptions
- ALT text: 80-150 words, detailed and vivid description suitable for accessibility
- Title: 25-60 characters, SEO-friendly and engaging
- Description: 100-200 words, rich and detailed description with atmosphere and context
- Keywords: 8-12 keywords separated by commas, relevant to the image
- Return ONLY valid JSON format, no additional text

Return format:
{
  "alt_text": "ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ëŒ€ì²´ í…ìŠ¤íŠ¸ (80-150 words, ìƒì„¸í•˜ê³  ìƒìƒí•œ ì„¤ëª…)",
  "title": "ì´ë¯¸ì§€ ì œëª© (25-60ì)",
  "description": "ì´ë¯¸ì§€ ìƒì„¸ ì„¤ëª… (100-200 words, í’ë¶€í•˜ê³  ë§¥ë½ì´ ìˆëŠ” ì„¤ëª…)",
  "keywords": "í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4, í‚¤ì›Œë“œ5, í‚¤ì›Œë“œ6, í‚¤ì›Œë“œ7, í‚¤ì›Œë“œ8"
}`
        },
        {
          role: "user",
          content: [
            {
              type: "text",
              text: `ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. ALT í…ìŠ¤íŠ¸, ì œëª©, ì„¤ëª…, í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.`
            },
            {
              type: "image_url",
              image_url: {
                url: imageUrl
              }
            }
          ]
        }
      ],
      response_format: { type: "json_object" },
      max_tokens: 800,
      temperature: 0.3
    });

    const content = response.choices[0].message.content.trim();
    let metadata;
    
    try {
      metadata = JSON.parse(content);
    } catch (error) {
      console.error('JSON íŒŒì‹± ì˜¤ë¥˜:', error);
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
      metadata = {
        alt_text: content.substring(0, 125),
        title: content.substring(0, 60),
        description: content,
        keywords: ''
      };
    }
    
    // AI ì‚¬ìš©ëŸ‰ ë¡œê¹…
    await logOpenAIUsage(
      'analyze-image-general',
      'general_image_analysis',
      response,
      {
        imageUrl: imageUrl,
        title: title,
        excerpt: excerpt
      }
    );

    console.log('âœ… ë²”ìš© ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ:', metadata);

    res.status(200).json({
      success: true,
      ...metadata,
      source: 'ai_analysis'
    });

  } catch (error) {
    console.error('âŒ ë²”ìš© ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì—ëŸ¬:', error);
    res.status(500).json({
      error: 'ë²”ìš© ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      details: error.message
    });
  }
}

